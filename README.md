# BA_03 Anomaly Detection(ì´ìƒì¹˜ íƒì§€)

<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/201814625-adf72ae0-4f94-4550-98f1-4986626f82b6.PNG">

ì´ìë£ŒëŠ” ê³ ë ¤ëŒ€í•™êµ ë¹„ë‹ˆì§€ìŠ¤ ì• ë„ë¦¬í‹±ìŠ¤ ê°•í•„ì„±êµìˆ˜ë‹˜ê»˜ ë°°ìš´ Anomaly Detection(ì´ìƒì¹˜ íƒì§€)ì„ ë°”íƒ•ìœ¼ë¡œ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.
ì´ë²ˆì—ëŠ” ì´ìƒì¹˜íƒì§€ë°©ì‹ì˜ ê¸°ë³¸ì ì¸ ê°œë…ê³¼ ê¸°ë²•ì¤‘ì—ì„œë„ Model-based learningì—ì„œ Auto encoderì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê³  Jupyter notebookì„ í†µí•´ ì§ì ‘ êµ¬í˜„ì„ í•´ë´„ìœ¼ë¡œì¨ ì´í•´ë¥¼ ë•ë„ë¡í•˜ê² ìŠµë‹ˆë‹¤.
## ëª©ì°¨
### 1. [BA_03 Anomaly Detection(ì´ìƒì¹˜ íƒì§€)_ê°œë…ì„¤ëª…](#ba_03-anomaly-detectionì´ìƒì¹˜-íƒì§€_ê°œë…ì„¤ëª…)
### 2. [BA_03 Anomaly Detection(ì´ìƒì¹˜ íƒì§€)_Model_based_learning Auto encoder](#ba_03-anomaly-detectionì´ìƒì¹˜-íƒì§€_model_based_learning-auto-encoder)
### 3. [BA_03 Anomaly Detection(ì´ìƒì¹˜ íƒì§€)_Model_based_learning Auto encoder ì‹¤ìŠµì½”ë“œ](ba_03-anomaly-detectionì´ìƒì¹˜-íƒì§€_model_based_learning-auto-encoder)
  #### 3.1. [ë°ì´í„° ì„¤ëª…](ë°ì´í„°_ì„¤ëª…)
  #### 3.2. [ì½”ë“œ](ì½”ë“œ)
  #### 3.3. [ê²°ë¡ ](ê²°ë¡ )

## BA_03 Anomaly Detection(ì´ìƒì¹˜ íƒì§€)_ê°œë…ì„¤ëª…
ì´ìƒì¹˜íƒì§€ëŠ” ê¸°ì¡´ ë°ì´í„°ì™€ëŠ” ë‹¤ë¥¸ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„°ì˜ ìƒ˜í”Œì…ë‹ˆë‹¤. ìœ„ ì†Œì œëª©ì—ì„œëŠ” ì´ìƒì¹˜íƒì§€ë¥¼ Anomaly Detectionì´ë¼ê³  ì†Œê°œí–ˆì§€ë§Œ ì‚¬ì‹¤ ì´ìƒì¹˜ëŠ” **Anomaly,Novelty**ë¡œë„ ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì‚´í‘œë³´ë©´ **Anomaly**ëŠ” ì•½ê°„ ë¶€ì •ì  ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ê³  ìˆê³  **Novelty**ëŠ” ê¸ì •ì ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ê³ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê¸°ì¤€ì€ ì–´ë– í•œ ë°ì´í„°ì— ì ìš©í•˜ë‚˜ì— ë”°ë¼ ë‹¤ë¥¼ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì˜ˆë¥¼ë“¤ë©´ ê³µì¥ì—ì„œëŠ” ì–‘í’ˆê³¼ ë¶ˆëŸ‰í’ˆì´ ì¡´ì¬í•˜ëŠ”ë° ê±°ê¸°ì„  ë¶ˆëŸ‰í’ˆì´ ë¶€ì •ì ì¸ ì˜ë¯¸ë¥¼ ë‚´í¬í•˜ê¸°ì— Anomalyë¼ëŠ” í‘œí˜„ì´ ì¢€ë” ì í•©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì£¼ì‹ì‹œì¥ì—ì„  ê°‘ìê¸° ê¸‰ë“±í•˜ëŠ” íŒ¨í„´ì´ ë¶„ì„í•´ì•¼í•˜ëŠ” ëŒ€ìƒì´ê¸°ì— ì´ëŸ´ë• ê¸ì •ì ì¸ Noveltyë¼ëŠ” ë‹¨ì–´ê°€ ì•Œë§ìŠµë‹ˆë‹¤.
  
ì—¬ê¸°ê¹Œì§€ ê¸€ì„ ì½ìœ¼ì‹  ë¶„ì€ ìƒê°í•˜ì‹¤ê²ë‹ˆë‹¤. ì´ìƒì¹˜íƒì§€ëŠ” ë°ì´í„°ê°€ ì–´ë–¤ ë°ì´í„°ëƒì— ë”°ë¼ ê¸ì •ì˜ ì˜ë¯¸ë„ ë¶€ì •ì˜ì˜ë¯¸ë„ ë ìˆ˜ ìˆê² êµ¬ë‚˜ ê·¸ëŸ¼ ì´ìƒì¹˜ ë°ì´í„°ëŠ” ë…¸ì´ì¦ˆ ë°ì´í„°ì¸ê°€?ë¼ê³  ê¶ê¸ˆì¦ì„ ê°€ì§ˆìˆ˜ìˆìŠµë‹ˆë‹¤. ì •ë‹µì€ Xì…ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ë…¸ì´ì¦ˆë°ì´í„°ëŠ” ë¬´ì‘ìœ„ì„±ì— ê¸°ë°˜í•œ ìì—°í˜„ìƒê°™ì€ ë°ì´í„°ì´ê³  ì´ìƒì¹˜ë°ì´í„°ëŠ” ì •ìƒë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì™€ì¤‘ì— ì–´ë–¤ê³¼ì •ì—ì„œ ìœ„ë°˜ë˜ì–´ ìƒì„±ë˜ëŠ”ê²ƒì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì•„ë˜ì— ê·¸ë¦¼ì„ ë³´ì‹œë©´ ì¡°ê¸ˆ ë” ì´í•´í•˜ê¸°ì— ìˆ˜ì›”í•˜ì‹¤ê²ë‹ˆë‹¤. (a)ê·¸ë¦¼ì— Aì ì€ ë‹¤ë¥¸ë°ì´í„°ë“¤ê³¼ ë–¨ì–´ì ¸ì„œ ë¶„í¬í•˜ê¸°ì— ì •ìƒë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì™€ì¤‘ì— ì–´ë–¤ê³¼ì •ì—ì„œ ìœ„ë°˜ë˜ì–´ ìƒì„±ë˜ì–´ì§„ ì´ìƒì¹˜ë°ì´í„°ì´ê³  (b)ê·¸ë¦¼ì— Aì ì€ ëª…í™•í•˜ê²Œ ë‹¤ë¥¸ë°ì´í„°ë“¤ê³¼ ë–¨ì–´ì ¸ì„œ ë¶„í¬í•œë‹¤ê³  ë§í• ìˆ˜ ì—†ê¸°ì— ë¬´ì‘ìœ„ì„±ì— ê¸°ë°˜í•œ ìì—°í˜„ìƒê°™ì€ ë…¸ì´ì¦ˆë°ì´í„°ë¼ê³  ë§í•©ë‹ˆë‹¤.
  
<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/202194818-fcf0b499-5123-42d7-ba6a-9d178511cbca.png">

ê·¸ëŸ¼ ë‚˜ì•„ê°€ì„œ **ì´ìƒì¹˜ íƒì§€ë¬¸ì œëŠ” ì´ìƒì¹˜ì¸ê²ƒê³¼ ì•„ë‹Œê²ƒì„ íŒë‹¨í•˜ëŠ”ê²ƒì´ë‹ˆ ë¶„ë¥˜ë¬¸ì œì¸ê°€?** ì´ë ‡ê²Œ ì§ˆë¬¸í• ìˆ˜ë„ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. í˜¹ì‹œ ì´ ì§ˆë¬¸ì— ëŒ€í•´ì„  ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?
ë‹¤ìŒ ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ë©´ì„œ í•œë²ˆ ìƒê°í•´ë³´ë©´ ì¢‹ì„ê²ƒê°™ìŠµë‹ˆë‹¤. 
<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/202180469-7e40b62a-0a72-43e8-b387-399c01b09d9f.png">


- ìœ„ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ë©´ Binary classification ê°™ì€ê²½ìš°ëŠ” A,Bë¼ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ ì–‘í’ˆ(o)ë¼ê³  ë¶„ë¥˜í• ìˆ˜ìˆìŠµë‹ˆë‹¤. 
- í•˜ì§€ë§Œ Anomaly detectionì—ì„œë„ ìƒˆë¡œìš´ ë°ì´í„° A,Bê°€ ë“¤ì–´ì˜¤ë©´ ë¶ˆëŸ‰í’ˆ(x)ë¼ê³  í•´ì•¼í• ê¹Œìš”? ì •ë‹µì€ Xì…ë‹ˆë‹¤. Anomaly detectionì—ì„œëŠ” ì–‘í’ˆ(o)ì´ ì•„ë‹ˆë‹¤.í•˜ê³  ì´ì•¼ê¸°í•´ì•¼ ì •í™•í•©ë‹ˆë‹¤. 

<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/202186745-6811d21f-ef0e-4c6c-8b24-8364a06c1c45.png">

- ìœ„ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ì‹œë©´ Classificationê³¼ Anomaly detectionì€ ì–‘í’ˆë°ì´í„°ê°€ ë¶ˆëŸ‰í’ˆë°ì´í„° ë³´ë‹¤ ìƒë‹¹íˆ ë§ë‹¤ëŠ” ê°€ì •í•˜ì—ì„œ trainingë°©ì‹ì—ì„œë„ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. Classificationì€ ì–‘í’ˆê³¼ ë¶ˆëŸ‰í’ˆ ë‘˜ë‹¤ train ì‹œí‚¤ì§€ë§Œ Anomaly detectionì—ì„œëŠ” ì–‘í’ˆë§Œ trainì„ ì‹œí‚µë‹ˆë‹¤. 

<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/202188297-8a249519-a4dd-4151-afe3-9c9f78d047c6.png">

ê·¸ëŸ¼ ì–¸ì œ Classficiationì„ ì‚¬ìš©í•´ì•¼í•˜ê³  ì–¸ì œ Anomaly detectionì„ ì‚¬ìš©í•˜ë©´ ì¢‹ì„ê¹Œìš”? ê°•í•„ì„±êµìˆ˜ë‹˜ê»˜ì„œ ê°€ë¥´ì³ì£¼ì‹ ë‚´ìš©ì— ë”°ë¼ ìœ„ ê·¸ë¦¼ì„ ì°¸ê³ í•˜ë©´ ì¢‹ì„ë“¯í•©ë‹ˆë‹¤.
ë³´í†µì€ ë°ì´í„°ì˜ imbalanceê°€ 1:99ì²˜ëŸ¼ ê·¹ì‹¬í•˜ê²Œ ë°œìƒ ë˜ê³  ì†Œìˆ˜ë²”ì£¼ì— ëŒ€í•œ ì˜ˆì‹œë“¤ì´ ì¶©ë¶„íˆ ìˆì§€ ì•Šìœ¼ë©´ ë³´í†µ ìµœí›„ì˜ ë³´ë¥˜ë¡œì¨ Anomaly detectionì„ ì‚¬ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë°ì´í„°ì˜ imblanceê°€ ì»¸ì–´ë„ ì†Œìˆ˜ë²”ì£¼ì— ëŒ€í•œ ì˜ˆì‹œë“¤ì´ ì¶©ë¶„íˆ ìˆìœ¼ë©´ SMOTEì™€ ê°™ì€ ì—¬ëŸ¬ê°€ì§€ samplingê¸°ë²•ì„ í†µí•´ ë°ì´í„°ì˜ balanceë¥¼ ë§ì¶”ì–´ ì¤ë‹ˆë‹¤. ì•„ë˜ì— A,Bë°ì´í„°ë¡œ ì˜ˆì‹œë¥¼ ë§Œë“¤ì–´ë³´ì•˜ëŠ”ë° ì°¸ê³ í•˜ì‹œë©´ ì¢‹ì„ê²ƒ ê°™ìŠµë‹ˆë‹¤.
||ì–‘í’ˆì˜ ê°¯ìˆ˜|ë¶ˆëŸ‰ì˜ ê°¯ìˆ˜|ê¸°ë²•|
|:---:|:---:|:---:|:---:|
| A |999,000|1,000|**Classification**|
| B |9,990|10|**Anomaly detection**|

## BA_03 Anomaly Detection(ì´ìƒì¹˜ íƒì§€)_Model_based_learning Auto encoder
  
ì´ìƒì¹˜ íƒì§€ê°€ ì–´ë–¤ê²ƒì¸ì§€ ì•„ì…¨ë‚˜ìš”? ê·¸ëŸ¼ ì´ë²ˆì—ëŠ” ëª¨ë¸ì„ í†µí•œ ì´ìƒì¹˜íƒì§€ê¸°ë²•ì¤‘ì— Auto encoderì— ëŒ€í•´ ì•Œì•„ë³´ë ¤ê³ í•©ë‹ˆë‹¤. 
  
<p align="center"><img width="500" alt="image" src="https://user-images.githubusercontent.com/97882448/202328901-561ae8f2-141d-48e6-b2d8-46e9bb843943.png">
<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/202329709-f4b95401-8be7-4202-87bf-c33de8366391.png">

ì˜¤í† ì¸ì½”ë”(AE)ëŠ” ì…ë ¥ì¸µê³¼ ì¶œë ¥ì´ ë™ì¼í•œ ì¸ê³µ ì‹ ê²½ë§ êµ¬ì¡°ì…ë‹ˆë‹¤. ë‹¨ ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µì˜ ì°¨ì›ì€ ë™ì¼í•˜ì§€ë§Œ ì€ë‹‰ì¸µì€ ì…ë ¥ì¸µì˜ ì°¨ì›ì˜ ìˆ˜ë¥¼ ë„˜ì–´ ì„¤ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‰½ê²Œ ì´ì•¼ê¸°í•˜ì—¬ì„œ ì½©ì‹¬ì€ë° ì½©ë‚˜ê³  íŒ¥ì‹¬ì€ë° íŒ¥ì´ë‚˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤. ê·¸ëŸ¼ ê¶ê¸ˆì¤‘ì´ ìƒê¸°ì‹¤í…ë° ì™œ ì½©ì„ ë„£ì–´ì„œ ì½©ì´ ë‚˜ì˜¤ëŠ” ëª¨ë¸ì„ ì™œ ì‚¬ìš©í• ê¹Œìš”? ì´ìœ ëŠ” í¬ê²Œ 2ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. 
1. ì°¨ì›ì¶•ì†Œì˜ ëª©ì ìœ¼ë¡œ AEë¥¼ í•™ìŠµì‹œì¼œ ì„±ëŠ¥ì„ í™•ì¸ í•œ ë’¤ ì ì¬ ë²¡í„°ì¸ featureë¥¼ ë‹¤ë¥¸ ê¸°ê³„í•™ìŠµëª¨í˜•ì˜ ì¸í’‹ìœ¼ë¡œ ì‚¬ìš©í•¨
2. ì…ë ¥ì •ë³´ì™€ AEì¶œë ¥ ì •ë³´ê°„ ì°¨ì´ë¥¼ ì´ìš©í•œ ë¶„ì„ì„ í†µí•´ ì´ìƒì¹˜ë¥¼ ë¶„ì„í•¨

<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/202330000-5d9bb67a-f862-4452-9c15-df7703a783fc.png">

AEì˜ êµ¬ì¡°ëŠ” Loss functionì€ Reproductionëœ ì¶œë ¥ ê°’ì—ì„œ ì…ë ¥ ê°’ì„ ëº€ ê°’ìœ¼ë¡œ ì´ë£¨ì–´ì§€ë©° anomaly scoreë¼ê³  ëª…ëª…ë˜ì–´ì§‘ë‹ˆë‹¤. ìœ„ì— ê·¸ë¦¼ì„ ì°¸ê³ í•˜ì‹œë©´ ì•„ê¹Œì „ì— ì´ì•¼ê¸°í•œê²ƒì²˜ëŸ¼ h(x)ëŠ” ğ‘¥ Ì‚  ì™€ ğ‘¥ì˜ ì°¨ì›ë³´ë‹¤ í¬ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤.  ì¦‰, h(x)ëŠ” ì¤‘ìš”í•œ ì…ë ¥ ê°’ì˜ ì •ë³´ë¥¼ ì¶•ì•½ í•´ì•¼í•©ë‹ˆë‹¤.  ê·¸ëŸ¬ë©´ ìœ„ ê·¸ë¦¼ì— without bottle layerì²˜ëŸ¼ h(x)ì˜ ì°¨ì› = ğ‘¥ ì˜ ì°¨ì›ê³¼ ê°™ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”? without bottle layerì²˜ëŸ¼ ì…ë ¥ì¸µ(=ì¶œë ¥ì¸µ)ê³¼  ì…ë ¥ì •ë³´ì™€ ì¶œë ¥ì •ë³´ë¥¼ íˆë“ ë ˆì´ì–´ì˜ ìˆ˜ì™€ ê°™ê²Œ ì„¤ì •ì„ í•˜ë©´ ì…ë ¥ì •ë³´ì™€ ì¶œë ¥ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì™¸ì›Œ ë²„ë¦¬ê¸°ì— overfittingì´ ë°œìƒë©ë‹ˆë‹¤. ë”°ë¼ì„œ With bottle layerì²˜ëŸ¼ ì…ë ¥ ì •ë³´ì™€ ì¶œë ¥ì •ë³´ë³´ë‹¤ íˆë“ ë ˆì´ì–´ìˆ˜ì˜ ì°¨ì›ì´ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤. 
  
<p align="center"><img width="600" alt="image" src="https://user-images.githubusercontent.com/97882448/202331854-8ddfd278-c22f-49c2-8ae8-bb54ea91036f.png">

ì´ì²˜ëŸ¼ AEëŠ” ì¤‘ìš” featureë§Œì„ ì••ì¶•í•˜ê¸°ì— ìš©ëŸ‰ë„ ì‘ê³  í’ˆì§ˆë„ ë” ì¢‹ìŠµë‹ˆë‹¤. ë˜í•œ ì°¨ì›ì˜ ì €ì£¼ë¥¼ ì˜ˆë°© í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¿ë§Œ ì•„ë‹ˆë¼ ìœ„ê·¸ë¦¼ì˜ ê¸°ì—¬ë„ë¥¼ ë³´ì‹œë©´ ë³µì›ì´ ì˜ ë˜ì§€ ì•Šì„ ê²½ìš°, ê¸°ì—¬ë„ì— ëŒ€í•œ ì°¨ì´ë„ ì•Œìˆ˜ ìˆìŠµë‹ˆë‹¤. AEë˜í•œ ë‹¨ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì…ë ¥ì— ëŒ€í•œ ì•½ê°„ì˜ ë³€í˜•ì—ë„ ëª¨ë¸ì´ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¨ì ì„ ë³´ì•ˆí•˜ê¸°ìœ„í•´ ì…ë ¥ì— noiseë¥¼ ì²¨ê°€í•´ noiseê°€ ì œê±°ëœ ê²°ê³¼ê°’ì´ ë‚˜ì˜¤ë„ë¡ í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ëª¨ë¸ì„ ë”ìš± robustí•˜ê²Œ ë§Œë“¤ë„ë¡ ë³´ì™„í•©ë‹ˆë‹¤. NoiseëŠ” ë³´í†µ Random Gaussian noiseê°€ ì‚¬ìš©ë©ë‹ˆë‹¤. 

## BA_03 Anomaly Detection(ì´ìƒì¹˜ íƒì§€)_Model_based_learning Auto encoder ì‹¤ìŠµì½”ë“œ

- ### ë°ì´í„° ì„¤ëª…
ì´ë²ˆì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” Kaggleì—ì„œ ì œê³µí•˜ëŠ” Credit Card Fraud Dectectionì…ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” 2013ë…„ 9ì›”ì— ì‹ ìš©ì¹´ë“œ ì‚¬ìš©ìë“¤ì˜ ì‹¤ì œ ê±°ë˜ê¸°ë¡ìœ¼ë¡œ ì´ 284,807 ê±´ì˜ ê±°ë˜ë‚´ì—­ì´ ì œê³µë©ë‹ˆë‹¤. ì´ ì¤‘ ì •ìƒê±°ë˜(Normal Transaction)ëŠ” 284,315ê±´ì´ê³  492ê±´ì´ ì‚¬ê¸° ê±°ë˜(Fraud Transaction)ì…ë‹ˆë‹¤. ì‚¬ê¸° ê±°ë˜ê°€ ì „ì²´ê±°ë˜ì— 0.172% ì°¨ì§€í•˜ë¯€ë¡œ ìœ„ì—ì„œ ì‚´í´ë³¸ ë‚´ìš©ì²˜ëŸ¼ ë§¤ìš° imbalanceí•œ íŠ¹ì§•ì„ ê°€ì§„ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

[Credit Card Fraud Dectection ë‹¤ìš´ë¡œë“œ](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

ë°ì´í„°ë“¤ì˜ featureëŠ” Time(ì‹œê°„), V1~V28(ê°œì¸ì •ë³´ë¡œ ì¸í•´ ê³µê°œë˜ì–´ì§€ì§€ ì•Šì€ê°’ì„ PCAë¡œ ë³€í™˜ëœ ê°’), Amount(ê±°ë˜ ê¸ˆì•¡), Class(ì‚¬ê¸°ì—¬ë¶€ë¡œ 1ì´ë©´ ì‚¬ê¸°ë¥¼ ë‹¹í–ˆê³  0ì´ë©´ ì •ìƒì„)ë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©° Nullê°’ì€ ì—†ëŠ” ë°ì´í„°ì´ë‹¤.

- ### ì½”ë“œ 

```python
#í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
import numpy as np
import pickle
import matplotlib.pyplot as plt
from scipy import stats
import tensorflow as tf
import seaborn as sns
from pylab import rcParams
from sklearn.model_selection import train_test_split
from keras.models import Model, load_model
from keras.layers import Input, Dense
from keras.callbacks import ModelCheckpoint, TensorBoard
from keras import regularizers

%matplotlib inline
# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •
sns.set(style='whitegrid', palette='muted', font_scale=1.5)
rcParams['figure.figsize'] = 14, 8
```
í•„ìš”í•œ ëª¨ë“ˆ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ê·¸ë˜í”„ì˜ ìŠ¤íƒ€ì¼ì´ë‹ˆ ìƒ‰ê¹”, ì‚¬ì´ì¦ˆë“±ì„ ë¯¸ë¦¬ ì„¤ì •í•œë‹¤.

```python
#ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  df.head()ë¥¼ í†µí•´ ë°ì´í„°ì˜ í˜•íƒœë¥¼ í™•ì¸í•´ë´„
df = pd.read_csv("creditcard.csv")
df.head()
```
<p align="center"><img width="680" alt="image" src="https://user-images.githubusercontent.com/97882448/202340245-bfe970b3-56e3-4569-8e79-0b5712cb1ac6.png">

```python
#df.shapeë¥¼í†µí•´ ë°ì´í„°ì˜ í˜•íƒœë¥¼ í™•ì¸í•¨
df.shape
#ë°ì´í„°ì— Nullê°’ì´ ìˆëŠ”ì§€ í™•ì¸í•´ë´„
df.isnull().values.any()
# RANDOM_SEEDì™€ Classì˜ LABELSì„¤ì •
RANDOM_SEED = 2022
LABELS = ["Normal", "Fraud"]
```
ë°ì´í„°ì˜ í˜•íƒœ ë° Nullê°’ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  Random_SEEDë¥¼ ì„¤ì •í•´ì¤Œ ì¶œë ¥ê°’ì€ ê°’ì´ ì²¨ë¶€í•œ Jupyter_Notebookì„ í†µí•´ í™•ì¸ê°€ëŠ¥í•¨

```python
count_classes = pd.value_counts(df['Class'])
#rotì€ ê¸€ìë¥¼ íšŒì „ ì‹œí‚´
count_classes.plot(kind = 'bar', rot=45, color="lightskyblue")
plt.title("Transaction class distribution")
plt.xticks(range(2), LABELS)
plt.xlabel("Class")
plt.ylabel("Frequency");
```
<p align="center"><img width="500" alt="image" src="https://user-images.githubusercontent.com/97882448/202346218-c8e15721-f581-477c-82bd-ae15b3381dc1.png">

bar plotìœ¼ë¡œ ê·¸ë ¤ë³´ë ¤ê³ í•˜ì§€ë§Œ Fraudì˜ ê°¯ìˆ˜ê°€ ë„ˆë¬´ ì‘ì•„ì„œ bar plotì€ ì¢‹ì€ê·¸ë¦¼ì´ ì•„ë‹Œê²ƒ ê°™ìŒ

```python
fraud = df[df.Class == 1]
normal = df[df.Class == 0]
fraud.shape #(492, 31)
normal.shape #(284315, 31)

#Classì˜ ê°¯ìˆ˜ë¥¼ ì„¸ìˆ˜ í‘œë¡œ ë§Œë“¦ ë˜í•œ reset_index()ë¥¼ í†µí•˜ì—¬ indexì— ëŒ€í•œ í‘œë„ ë§Œë“¤ì–´ì¤Œ 
table = df['Class'].value_counts().to_frame().reset_index()
# ì „ì²´ì˜ ë°ì´í„°ì—ì„œ ëª‡ í”„ë¡œë¥¼ ì°¨ì§€ í•˜ëŠ”ì§€ í‘œë¥¼ ì¶”ê°€í•˜ê³  ì†Œìˆ˜ì  4ì§¸ìë¦¬ê¹Œì§€ ì¶”ì¶œ
table['Percent(%)'] = df["Class"].apply(lambda x : round(100*float(x) / len(data), 4))
#indexì™€ Classì˜ ì´ë¦„ì„ ë°”ê¾¸ì–´ì¤Œ
table= table.rename(columns = {"index" : "Target", "Class" : "Count"})

table
```
ê°¯ìˆ˜ë¥¼ ë³´ì•„í•˜ë‹ˆ ì™œ barplotì— fraudê°€ ì˜ ë‚˜ì˜¤ì§€ ì•Šì•˜ëŠ”ì§€ ì•Œê²ƒ ê°™ìŒ ë”°ë¼ì„œ tableì„ ìƒì„±í•´ë³´ë‹ˆ ì•„ë˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ì‚¬ê¸°ê±°ë˜ì˜ ë¹„ìœ¨ì´ 0.1727% ì˜€ë˜ê²ƒì„ ì•Œìˆ˜ ìˆìŒ
  
<p align="center"><img width="250" alt="image" src="https://user-images.githubusercontent.com/97882448/202347439-16aeef42-af79-454c-98a4-3e6bcedd830f.png">

```python
#ì‚¬ê¸°ê±°ë˜ ê¸ˆì•¡ì˜ ë¶„í¬
frauds.Amount.describe()
```
<p align="center"><img width="251" alt="image" src="https://user-images.githubusercontent.com/97882448/202348405-5e9b0900-092c-4c43-a648-985762d9f58d.png">
  
```python
#ì •ìƒê±°ë˜ ê¸ˆì•¡ì˜ ë¶„í¬
normal.Amount.describe()
```
<p align="center"><img width="262" alt="image" src="https://user-images.githubusercontent.com/97882448/202348460-6888732c-55f7-4fea-9f24-2d57d3951ee2.png">

```python
#ì •ìƒê³¼ ì‚¬ê¸°ê±°ë˜ì˜ ê±°ë˜ëŸ‰ê³¼ ê¸ˆì•¡ì„ graphë¡œ ë‚˜íƒ€ëƒ„
bins = np.linspace(200, 2500, 200)
plt.hist(normal.Amount, bins=bins, alpha=1, density=True, label='Normal')
plt.hist(frauds.Amount, bins=bins, alpha=1, density=True, label='Fraud')
plt.legend(loc='upper right')
plt.title("Transaction amount and Percentage of transactions")
plt.xlabel("Transaction amount (USD)")
plt.ylabel("Percentage of transactions");
plt.show()
```
<p align="center"><img width="700" alt="image" src="https://user-images.githubusercontent.com/97882448/202349441-b84e1e14-4cd5-4703-a606-2e3fde8b4103.png">
  
ì •ìƒê³¼ ì‚¬ê¸°ê±°ë˜ì˜ ê±°ë˜ëŸ‰ê³¼ ê¸ˆì•¡ì„ í•˜ë‚˜ì˜ ê·¸ë˜í”„ë¡œ ê²¹ì³ì„œ ê·¸ë ¤ë´„ xì¶•ì„ ê±°ë˜ê¸ˆì•¡ìœ¼ë¡œ ì„¤ì •í•˜ê³  yì¶•ì„ ê±°ë˜íšŸìˆ˜ì˜ %ì¸ë° í™•ì‹¤íˆ ì°¨ì´ê°€ ë‚œë‹¤ê³  ë³¼ìˆ˜ìˆìŒ

```python 
#ì •ìƒê±°ë˜ì™€ ì‚¬ê¸°ê±°ë˜ì˜ ì‹œê°„(ì´ˆ)ì— ë”°ë¥¸ ê±°ë˜ëŸ‰ì„ ì•Œì•„ë³´ê³ ìí•¨
#sub plotì´ 2ê°œì˜ graphê°€ ê·¸ë¦´ìˆ˜ìˆë„ë¡ ì„¤ì •í•´ì£¼ê³   xì¶•ì„ ì„¤ì •í•¨
function,(ax1, ax2) = plt.subplots(2, 1, sharex=True)
function.suptitle('Time of transaction and Amount by class')
#ì‹œê°„ì— ë”°ë¥¸ ì‚¬ê¸°ê±°ë˜ì˜ ê±°ë˜ëŸ‰
ax1.scatter(frauds.Time, frauds.Amount,color="red")
ax1.set_title('Fraud Class')
#ì‹œê°„ì— ë”°ë¥¸ ì •ìƒê±°ë˜ì˜ ê±°ë˜ëŸ‰
ax2.scatter(normal.Time, normal.Amount,color="skyblue")
ax2.set_title('Normal Class')

plt.xlabel('Time_(Sec)')
plt.ylabel('Amount')
plt.show()
```
<p align="center"><img width="700" alt="image" src="https://user-images.githubusercontent.com/97882448/202351958-185b9e8e-b26d-443e-b8d2-866b7dcac6ae.png">
  
ì •ìƒê±°ë˜ì™€ ì‚¬ê¸°ê±°ë˜ì˜ ì‹œê°„(ì´ˆ)ì— ë”°ë¥¸ ê±°ë˜ëŸ‰ì„ ì•Œì•„ë³´ë ¤ê³  xì¶•ì„ ê³µìœ í•˜ì—¬ ì‚¬ê¸°ì™€ ì •ìƒê±°ë˜ê°€ ì–´ë–¤ ë¶€ë¶„ì´ ë‹¤ë¥¸ì§€ ì•Œì•„ë³´ê³ ìí•˜ì˜€ìœ¼ë‚˜ ê²°ê³¼ì ìœ¼ë¡œ, ì •ìƒê±°ë˜ë‚˜ ì‚¬ê¸°ê±°ë˜ë‚˜ ì‹œê°„ì— ë¹„ë¡€í•´ì„œ ì¢‹ì•„ë³´ì´ì§„ ì•Šì•„ë³´ì„

```python 
from sklearn.preprocessing import StandardScaler
#ì‹œê°„ì´ ê·¸ë ‡ê²Œ ì¤‘ìš”í•œ ìš”ì†Œê°€ ì•„ë‹ˆë¼ íŒë‹¨ë˜ì–´ ì‹œê°„ì„ ì§€ì›€ 
data = df.drop(['Time'], axis=1)
#ê±°ë˜ëŸ‰ì„ StandardScalerë¥¼ í†µí•´ ê°’ë“¤ì„ ìŠ¤ì¼€ì¼ë§ í•¨, ì´ìœ ëŠ” í‰ê· ì„ ì œê±°í•˜ê³  ë°ì´í„°ë¥¼ ë‹¨ìœ„ ë¶„ì‚°ìœ¼ë¡œ ì¡°ì •í•˜ê¸°ì— ì´ìƒì¹˜ê°€ ìˆë‹¤ë©´ ë°ì´í„°ì˜ í™•ì‚°ì€ ë§¤ìš° ë‹¬ë¼ì ¸ì„œ ì´ìƒì¹˜ì— ë§¤ìš° ë¯¼ê°
data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))
```
ë¨¼ì €, ì‹œê°„ì´ ì´ìƒì¹˜íƒì§€ì— ê·¸ë ‡ê²Œ ì¤‘ìš”í•œ ìš”ì†Œê°€ ì•„ë‹ˆë¼ê³  ìƒê°ë˜ì–´ ì‹œê°„ì—´ì„  dropí•˜ê³  amountì¸ ê±°ë˜ëŸ‰ì„ StandardScalerë¥¼ í†µí•´ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ ë°ì´í„°ê°€ ì´ìƒì¹˜ì— ë§¤ìš° ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ë„ë¡ ë§Œë“¦
  
```python  
X_train, X_test = train_test_split(data, test_size=0.3, random_state=RANDOM_SEED)
#ì´ìƒì¹˜íƒì§€ì—ëŠ” ì¤‘ìš”í•œíŠ¹ì„±ì´ ìˆëŠ”ë° trainë°ì´í„°ëŠ” ì •ìƒì¸ ë°ì´í„°ë§Œ ì‚¬ìš©í•¨
X_train = X_train[X_train.Class == 0]
X_train = X_train.drop(['Class'], axis=1)

y_test = X_test['Class']
X_test = X_test.drop(['Class'], axis=1)

X_train = X_train.values
X_test = X_test.values

X_train.shape
```
ì´ì œ ë°ì´í„°ë“¤ì„ í›ˆë ¨ì‹œì¼œì•¼ í•¨ ì´ìƒì¹˜íƒì§€í• ë•Œ ì¤‘ìš”í•œìš”ì†Œê°€ ìˆëŠ”ë° ê·¸ê²ƒì¤‘ í•˜ë‚˜ëŠ” **trainë°ì´í„°ëŠ” ì •ìƒ ë°ì´í„°**ë§Œ ì‚¬ìš©í•´ì•¼í•¨
test_sizeë¥¼ 0.3ìœ¼ë¡œ ì„¤ì •í•˜ê³  random_stateëŠ” ì•„ê¹Œ ì„¤ì •í•œ 2022ë¡œ ì„¤ì •ë¨ ê·¸ëŸ¬ë©´ X_train.shapeì€ (199000, 29)ë¡œ ì„¤ì •ë¨
  
```python   
#X_trainì˜ ì—´ì˜ ê°¯ìˆ˜:28ê°œ
input_dim = X_train.shape[1]

encoding_dim = 14

input_layer = Input(shape=(input_dim, ))
#regularizersë¥¼ L1ìœ¼ë¡œ ì„¤ì • í•˜ì˜€ìŒ
encoder = Dense(encoding_dim, activation="tanh", 
                activity_regularizer=regularizers.l1(10e-5))(input_layer)
encoder = Dense(int(encoding_dim / 2), activation="relu")(encoder)
decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)
decoder = Dense(input_dim, activation='relu')(decoder)
autoencoder = Model(inputs=input_layer, outputs=decoder)
autoencoder.summary()
```
<p align="center"><img width="578" alt="image" src="https://user-images.githubusercontent.com/97882448/202365189-72e2aa85-c61a-4cc4-a925-ba496ef91c17.png">

ìœ„ì— AutoencoderëŠ” 4ê°œì˜ fully connected layerë¡œ ë§Œë“¤ì–´ì ¸ ìˆìœ¼ë©°, ê° layerëŠ” 14, 7, 7, 29ê°œë¡œ êµ¬ì„±ë¨ 
ëª¨ë¸ì„ ìš”ì•½í•˜ë©´ ìœ„ì™€ ê°™ì€ê²°ê³¼ê°€ ë‚˜ì˜´

```python   
nb_epoch = 50
batch_size = 32
#ì˜¤í† ì¸ì½”ë” ì»´íŒŒì¼í•¨
autoencoder.compile(optimizer='adam', 
                    loss='mean_squared_error', 
                    metrics=['accuracy'])
#ModelCheckpointë¡œ ì„±ëŠ¥ ì¢‹ì€ëª¨ë¸ì„ ì €ì¥í•¨
checkpointer = ModelCheckpoint(filepath="model.h",
                               verbose=0,
                               save_best_only=True)
#TensorBoardëŠ” TensorFlowì—ì„œ ë°œìƒí•œ ë¡œê·¸ë¥¼ í‘œì‹œí•¨
tensorboard = TensorBoard(log_dir='./logs',
                          histogram_freq=0,
                          write_graph=True,
                          write_images=True)

history = autoencoder.fit(X_train, X_train,
                    epochs=nb_epoch,
                    batch_size=batch_size,
                    shuffle=True,
                    validation_data=(X_test, X_test),
                    verbose=1,
                    callbacks=[checkpointer, tensorboard]).history
```
<p align="center"><img width="900" alt="image" src="https://user-images.githubusercontent.com/97882448/202366748-3887551d-32a6-432b-93d2-ab2a38b01111.png">

```python 
#ë°ì´í„° ë¡œë“œ
autoencoder = load_model('model.h')
#epochì— ë”°ë¥¸ trainê³¼ testì˜ lossí•¨ìˆ˜
plt.plot(history['loss'])
plt.plot(history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right');
```
<p align="center"><img width="900" alt="image" src="https://user-images.githubusercontent.com/97882448/202391533-12fe581d-4e52-4c08-af2b-2a749a371b05.png">
  
ë°ì´í„°ë¥¼ ë¡œë“œ í•œë‹¤ìŒì— ìœ„ í‘œë¥¼ ë³´ë©´ epochì— ë”°ë¥¸ trainê³¼ testì˜ lossí•¨ìˆ˜ì„
  
```python 
predictions = autoencoder.predict(X_test)
#np.power(a,b)ëŠ” ì œê³±ì—°ì‚°ì„ í• ë•Œ ì‚¬ìš©ë˜ë©´ a^bë¥¼ ëœ»í•¨
mse = np.mean(np.power(X_test - predictions, 2), axis=1)
error_df = pd.DataFrame({'reconstruction_error': mse,
                        'true_class': y_test})
percent=[0.001,0.1,0.9,0.999]
error_df.describe(percentiles=percent)
```
<p align="center"><img width="350" alt="image" src="https://user-images.githubusercontent.com/97882448/202398829-6b707f5d-215d-40ea-be0f-2d14da32ec65.png">

ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ê°€ ì •ìƒ ë°ì´í„°ì´ê¸°ì— reconstruction_errorì˜ í‰ê· ë„ 0.736549ë°–ì— ë˜ì§€ ì•Šìœ¼ë‚˜ 99.9%ëŠ” 44.121528ë¡œ ì‚¬ê¸°ê±°ë˜ë¼ê³  ë‚˜ì˜´
 
```python 
# ì •ìƒë°ì´í„°ì˜ reconstruction_errorë¶„í¬
fig = plt.figure()
#111ì€ 1í–‰ì§¸ì˜ 1ì—´ì˜ ì²« ë²ˆì§¸ë¼ëŠ” ì˜ë¯¸ì„
ax = fig.add_subplot(111)
normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 20)]
normal_error_df= ax.hist(normal_error_df.reconstruction_error.values, bins=30, color = "skyblue")
```
<p align="center"><img width="450" alt="image" src="https://user-images.githubusercontent.com/97882448/202396791-24e43a2f-a2e4-4eae-acf4-37113207e563.png">
  
ì •ìƒë°ì´í„°ì˜ reconstruction_errorë¶„í¬ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ìŒ
 
```python 
# ì‚¬ê¸°ë°ì´í„°ì˜ reconstruction_errorë¶„í¬
fig = plt.figure()
ax = fig.add_subplot(111)
fraud_error_df = error_df[error_df['true_class'] == 1]
fraud_error_df = ax.hist(fraud_error_df.reconstruction_error.values, bins=30 ,color = "red")
```  
<p align="center"><img width="450" alt="image" src="https://user-images.githubusercontent.com/97882448/202399705-a624b26e-07a9-494e-a197-cefc9bd3d591.png">
  
ì‚¬ê¸°ë°ì´í„°ì˜ reconstruction_errorë¶„í¬ëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ìŒ
 
```python 
#sklearnì˜ ê¸°ë²•ë“¤ 
from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,
                             roc_curve, recall_score, classification_report, f1_score,
                             precision_recall_fscore_support)
fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)
roc_auc = auc(fpr, tpr)

plt.title('ROC')
#AUROCë¥¼ ê³„ì‚°
plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)
plt.legend(loc='lower right')
plt.plot([0,1],[0,1],'r--')
plt.xlim([-0.01, 1])
plt.ylim([0, 1.01])
plt.ylabel('True Positive Rate(TPR)')
plt.xlabel('False Positive Rate(FPR)')
plt.show();

precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)
plt.plot(recall, precision, 'pink', label='Precision-Recall curve')
plt.title('Recall vs Precision')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.show()

plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')
plt.title('Precision for different threshold values')
plt.xlabel('Threshold')
plt.ylabel('Precision')
plt.show()
  
plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')
plt.title('Recall for different threshold values')
plt.xlabel('Reconstruction error')
plt.ylabel('Recall')
plt.show()
```
<p align="center"><img width="1000" alt="image" src="https://user-images.githubusercontent.com/97882448/202400810-310a38e9-1724-4c93-8577-215ed295cab4.png">

ìœ„ ê·¸ë¦¼ì„ ë³´ë©´ Reconstruction errorê°€ ì¦ê°€í• ìˆ˜ë¡ ì •ë°€ë„ê°€ ì˜¬ë¼ê°€ëŠ” ê²ƒì„ ì•Œìˆ˜ ìˆìŒ

```python 
threshold = 3
groups = error_df.groupby('true_class')
fig, ax = plt.subplots()

for name, group in groups:
    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',
            label= "Fraud" if name == 1 else "Normal")
ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')
ax.legend()
plt.title("Reconstruction error for different classes")
plt.ylabel("Reconstruction error")
plt.xlabel("Data point index")
plt.show();
```
![image](https://user-images.githubusercontent.com/97882448/202403830-cd68a650-5c4a-42b5-a96c-7cef31998fa3.png)

```python 
y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]
conf_matrix = confusion_matrix(error_df.true_class, y_pred)
plt.figure(figsize=(12, 12))
sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");
plt.title("Confusion matrix")
plt.ylabel('True class')
plt.xlabel('Predicted class')
plt.show()
```
<p align="center"><img width="500" alt="image" src="https://user-images.githubusercontent.com/97882448/202404021-162db7c1-56aa-494a-9f5c-58eb9ed4cef0.png">

threshold=3ì— ë”°ë¥¸ confusion matrixì„ ì‹¤ì œ ì‚¬ê¸°ê±°ë˜ì˜€ì„ë•Œ ì •ìƒì´ë¼ê³  í•˜ëŠ” ìƒí™©ì„ ìµœëŒ€í•œ ì¤„ì´ê¸° ìœ„í•´ì„œ thresholdë¥¼ ìƒê°ë³´ë‹¤ ë‚®ê²Œ ì¡ì•˜ìŒ

- ### ê²°ë¡ 
  
ì—¬ê¸°ì„œ ì‚¬ìš©í•œ ë°ì´í„°ì…‹ì€ imblance í•˜ê¸°ì— ë†’ì€ ì •í™•ë„(accuracy)ë§Œ ë³´ë©´ ì¢‹ì€ ëª¨ë¸ì´ë¼ê³  ì°©ê°í• ìˆ˜ ìˆì§€ë§Œ ë‚®ì€ ì¬í˜„ë¥ (recall)ê³¼ ì •ë°€ë„(precision)ë¥¼ ë³´ì„
recallê³¼ precisionì„ ê°œì„ í•˜ê¸° ìœ„í•œ ê²ƒìœ¼ë¡œëŠ” ë‹¤ë¥¸ ì´ìƒì¹˜íƒì§€ë°©ë²•ì´ë‚˜ AEì˜ êµ¬ì¡°ë¥¼ ë°”ê¿”ë³´ë©´ ì¢‹ì„ê²ƒ ê°™ìŒ 
  
---
 ### Reference
 1. https://sustaining-starflower-aff.notion.site/2022-2-0e068bff3023401fa9fa13e96c0269d7 <ê°•í•„ì„±êµìˆ˜ë‹˜ ìë£Œ>
 2. https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd <Credit Card Fraud Detection using Autoencoders in Keras >
